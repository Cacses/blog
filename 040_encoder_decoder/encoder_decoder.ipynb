{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yg90deR13qYE"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/040_encoder_decoder/encoder_decoder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Kzp2jHl3qY7"
   },
   "source": [
    "# La arquitectura *Encoder-Decoder*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En posts anteriores hemos visto como podemos utilizar `redes neuronales recurrentes` para [generación de texto](https://sensioai.com/blog/037_charRNN) así como [clasificación de texto](https://sensioai.com/blog/038_clasificacion_texto). En ambas aplicaciones hemos entrenado una red neuronal que alimentamos con una secuencia de texto, ya sean letras o palabras en una frase, a la cual le pedimos a la salida una distribución de probabilidad sobre la diferentes categorías (para el caso de la clasificación) o directamente el vocabulario (para la generación de texto). La principal limitación de estos modelos es que no podemos obtener más que una salida, y es por esto que en el caso de la generación de texto concatenamos la salida en cada instante a las entradas para utilizarlo de nuevo como entradas y obtener así una nueva predicción. En este post vamos a ver cómo podemos implementar modelos que no sólo sean capaces de recibir secuencias a la entrada, sino que también puedan dar secuencias de longitud arbitraria a la salida. Este tipo de modelos se conocen como modelos *sequence to sequence* (o simplemente *seq2seq*) y pueden ser utilizados para tareas tales como la generación de texto, traducción entre idiomas, resumir textos, etc. \n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
    "\n",
    "Un modelo *seq2seq* está formado por dos partes: un `encoder` que recibe la secuencia de entrada y un `decoder` encargado de generar la secuencia de salida. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 💡 Este post está basado en el siguiente [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), en el que podrás encontrar más información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El *dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T08:31:48.390280Z",
     "start_time": "2020-09-03T08:31:48.382280Z"
    }
   },
   "source": [
    "En este post vamos a ver cómo entrenar este tipo de arquitectura para traducir texto del inglés al castellano. Puedes encontrar este dataset para traducción [aquí](https://www.manythings.org/anki/). Una vez descargados los datos vamos a leer el archivo, separando los pares de frases de cada ejemplo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T08:47:45.369925Z",
     "start_time": "2020-09-03T08:47:45.354907Z"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def read_file(file, reverse=False):\n",
    "    # Read the file and split into lines\n",
    "    lines = open(file, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T08:47:48.591935Z",
     "start_time": "2020-09-03T08:47:45.370926Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs = read_file('spa.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T08:49:19.696299Z",
     "start_time": "2020-09-03T08:49:19.689299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tom is still working here isn t he ?',\n",
       " 'tom todavia esta trabajando aca no ?']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya hemos visto en los posts anteriores, necesitamos un `tokenizer`. En este caso, la clase `Lang` se encargará de asignar índice a cada palabra calculando también su frecuencia para, más tarde, poder quedarnos sólo con las palabras más frecuentes. Necesitaremos, además, dos nuevos *tokens* especiales: el token `<eos>` y el token `<sos>` para indicar, respectivamente, el inicio y final de una frase. Más adelante veremos cómo utilizarlos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:02:56.995963Z",
     "start_time": "2020-09-03T09:02:56.985964Z"
    }
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def indexesFromSentence(self, sentence):\n",
    "        return [self.word2index[word] for word in sentence.split(' ')]\n",
    "    \n",
    "    def sentenceFromIndex(self, index):\n",
    "        return [self.index2word[ix] for ix in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opcionalmente, también podemos a indicar la longitud máxima de las frases a utilizar así como un conjunto de comienzos de frases que queramos filtrar. Esto lo hacemos únicamente para acelerar el proceso de entrenamiento, trabajando con un conjunto pequeño de datos. Si quieres entrenar un buen traductor no hace falta que filtres los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:02:57.263653Z",
     "start_time": "2020-09-03T09:02:57.245654Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p, lang):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[lang].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs, lang=0):\n",
    "    return [pair for pair in pairs if filterPair(pair, lang)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:03:00.694526Z",
     "start_time": "2020-09-03T09:02:57.387326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos 124547 pares de frases\n",
      "Filtramos a 7685 pares de frases\n",
      "Longitud vocabularios:\n",
      "spa 2685\n",
      "eng 3883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i m clumsy .', 'soy torpe .']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepareData(file, filter=False, reverse=False):\n",
    "    \n",
    "    pairs = read_file(file, reverse)\n",
    "    print(f\"Tenemos {len(pairs)} pares de frases\")\n",
    "    \n",
    "    \n",
    "    if filter:\n",
    "        pairs = filterPairs(pairs, int(reverse))\n",
    "        print(f\"Filtramos a {len(pairs)} pares de frases\")\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang('eng')\n",
    "        output_lang = Lang('spa')\n",
    "    else:\n",
    "        input_lang = Lang('spa')\n",
    "        output_lang = Lang('eng')\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "                           \n",
    "    print(\"Longitud vocabularios:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "                           \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('spa.txt', True, False)\n",
    "                           \n",
    "random.choice(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez construidos los dos vocabularios, podemos obtener los índices a partir de una frase, y viceversa, de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:03:56.418587Z",
     "start_time": "2020-09-03T09:03:56.412584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[92, 50, 2459, 115, 718, 4]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.indexesFromSentence('voy a necesitar tu ayuda .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T09:03:56.560994Z",
     "start_time": "2020-09-03T09:03:56.546994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['voy', 'a', 'necesitar', 'tu', 'ayuda', '.']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.sentenceFromIndex([92, 50, 2459, 115, 718, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para terminar, las siguientes clases se encargarán de alimentar nuestro modelo *seq2seq* utilizando las clases `Dataset` y `DataLoader` de `Pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "nlp_transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3ddabfba841642efa8c92aa0fa4cabf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "482d8ecf35a44521945b1f760ee31021": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6019610946264ce1a7a2e79cfcd82fa5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fbe31372cd24cdd9cf2ac44ad3067e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e370cbea737a47e8bb35feba70b04cb8",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ddabfba841642efa8c92aa0fa4cabf8",
      "value": 231508
     }
    },
    "922443b940dc4237b7d3bd7efff5ea3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6019610946264ce1a7a2e79cfcd82fa5",
      "placeholder": "​",
      "style": "IPY_MODEL_c79bf36cd9b34317a6b6c5905ab362bd",
      "value": " 232k/232k [00:30&lt;00:00, 7.64kB/s]"
     }
    },
    "c79bf36cd9b34317a6b6c5905ab362bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e370cbea737a47e8bb35feba70b04cb8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efedcba8722e430aa42a49fadd498011": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6fbe31372cd24cdd9cf2ac44ad3067e8",
       "IPY_MODEL_922443b940dc4237b7d3bd7efff5ea3a"
      ],
      "layout": "IPY_MODEL_482d8ecf35a44521945b1f760ee31021"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
