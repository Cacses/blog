{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yg90deR13qYE"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/040_encoder_decoder/encoder_decoder.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Kzp2jHl3qY7"
   },
   "source": [
    "# La arquitectura *Encoder-Decoder*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En posts anteriores hemos visto como podemos utilizar `redes neuronales recurrentes` para [generación de texto](https://sensioai.com/blog/037_charRNN) así como [clasificación de texto](https://sensioai.com/blog/038_clasificacion_texto). En ambas aplicaciones hemos entrenado una red neuronal que alimentamos con una secuencia de texto, ya sean letras o palabras en una frase, a la cual le pedimos a la salida una distribución de probabilidad sobre la diferentes categorías (para el caso de la clasificación) o directamente el vocabulario (para la generación de texto). La principal limitación de estos modelos es que no podemos obtener más que una salida, y es por esto que en el caso de la generación de texto concatenamos la salida en cada instante a las entradas para utilizarlo de nuevo como entradas y obtener así una nueva predicción. En este post vamos a ver cómo podemos implementar modelos que no sólo sean capaces de recibir secuencias a la entrada, sino que también puedan dar secuencias de longitud arbitraria a la salida. Este tipo de modelos se conocen como modelos *sequence to sequence* (o simplemente *seq2seq*) y pueden ser utilizados para tareas tales como la generación de texto, traducción entre idiomas, resumir textos, etc. \n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 💡 Este post está basado en el siguiente [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), en el que podrás encontrar más información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El *dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T08:31:48.390280Z",
     "start_time": "2020-09-03T08:31:48.382280Z"
    }
   },
   "source": [
    "En este post vamos a ver cómo entrenar este tipo de arquitectura para traducir texto del inglés al castellano. Puedes encontrar un ejemplo de dataset para traducción [aquí](https://www.manythings.org/anki/). Una vez descargados los datos vamos a leer el archivo, separando los pares de frases de cada ejemplo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:48:06.955581Z",
     "start_time": "2020-09-04T09:48:06.940299Z"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def read_file(file, reverse=False):\n",
    "    # Read the file and split into lines\n",
    "    lines = open(file, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:48:10.469060Z",
     "start_time": "2020-09-04T09:48:07.059034Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs = read_file('spa.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:48:10.485059Z",
     "start_time": "2020-09-04T09:48:10.470060Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last summer i worked part time on a farm .',\n",
       " 'el verano pasado trabajaba media jornada en una granja .']"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya hemos visto en los posts anteriores, necesitamos un `tokenizer`. En este caso, la clase `Lang` se encargará de asignar un índice único a cada palabra calculando también su frecuencia para, más tarde, poder quedarnos sólo con las palabras más frecuentes. Necesitaremos, además, dos nuevos *tokens* especiales: el token `<eos>` y el token `<sos>` para indicar, respectivamente, el inicio y final de una frase. Más adelante veremos cómo utilizarlos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:48:10.500800Z",
     "start_time": "2020-09-04T09:48:10.486060Z"
    }
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"SOS\": 0, \"EOS\": 1, \"PAD\": 2}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
    "        self.n_words = 3  # Count SOS, EOS and PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def indexesFromSentence(self, sentence):\n",
    "        return [self.word2index[word] for word in sentence.split(' ')]\n",
    "    \n",
    "    def sentenceFromIndex(self, index):\n",
    "        return [self.index2word[ix] for ix in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opcionalmente, también podemos indicar la longitud máxima de las frases a utilizar así como un conjunto de comienzos de frases que queramos filtrar. Esto lo hacemos únicamente para acelerar el proceso de entrenamiento, trabajando con un conjunto pequeño de datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T10:17:18.777294Z",
     "start_time": "2020-09-04T10:17:18.768017Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p, lang, filters, max_length):\n",
    "    return len(p[0].split(' ')) < max_length and \\\n",
    "        len(p[1].split(' ')) < max_length and \\\n",
    "        p[lang].startswith(filters)\n",
    "\n",
    "def filterPairs(pairs, filters, max_length, lang=0):\n",
    "    return [pair for pair in pairs if filterPair(pair, lang, filters, max_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T10:17:26.493302Z",
     "start_time": "2020-09-04T10:17:18.948647Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos 124547 pares de frases\n",
      "Filtramos a 7685 pares de frases\n",
      "Longitud vocabularios:\n",
      "spa 2686\n",
      "eng 3884\n",
      "Tenemos 124547 pares de frases\n",
      "Longitud vocabularios:\n",
      "spa 13074\n",
      "eng 25274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tom and i were married to each other for three years . EOS',\n",
       " 'tomas y yo estuvimos casados durante tres anos . EOS']"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepareData(file, filters=None, max_length=None, reverse=False):\n",
    "    \n",
    "    pairs = read_file(file, reverse)\n",
    "    print(f\"Tenemos {len(pairs)} pares de frases\")\n",
    "    \n",
    "    if filters is not None:\n",
    "        assert max_length is not None\n",
    "        pairs = filterPairs(pairs, filters, max_length, int(reverse))\n",
    "        print(f\"Filtramos a {len(pairs)} pares de frases\")\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang('eng')\n",
    "        output_lang = Lang('spa')\n",
    "    else:\n",
    "        input_lang = Lang('spa')\n",
    "        output_lang = Lang('eng')\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "        \n",
    "        # add <eos> token\n",
    "        pair[0] += \" EOS\"\n",
    "        pair[1] += \" EOS\"\n",
    "                           \n",
    "    print(\"Longitud vocabularios:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "                           \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('spa.txt')\n",
    "\n",
    "# descomentar para usar el dataset filtrado\n",
    "#input_lang, output_lang, pairs = prepareData('spa.txt', filters=eng_prefixes, max_length=MAX_LENGTH)\n",
    "                           \n",
    "random.choice(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez construidos los dos vocabularios, podemos obtener los índices a partir de una frase, y viceversa, de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T10:18:20.417278Z",
     "start_time": "2020-09-04T10:18:20.409768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[68, 5028, 135, 4]"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.indexesFromSentence('tengo mucha sed .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T10:18:27.565641Z",
     "start_time": "2020-09-04T10:18:27.546975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tengo', 'mucha', 'sed', '.']"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.sentenceFromIndex([68, 5028, 135, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para terminar, las siguientes clases se encargarán de alimentar nuestro modelo *seq2seq* utilizando las clases `Dataset` y `DataLoader` de `Pytorch`. Debido a que nuestras frases pueden tener diferentes longitudes, tenemos que asegurarnos que al construir nuestros batches todas tengan la misma longitud, ya que para alimentar la red necesitamos tensores rectangulares. Esto lo conseguimos con la función `collate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:48:46.538721Z",
     "start_time": "2020-09-04T09:48:46.510182Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99637, 24910)"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_lang, output_lang, pairs):\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        self.pairs = pairs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "        \n",
    "    def __getitem__(self, ix):\n",
    "        return torch.tensor(self.input_lang.indexesFromSentence(self.pairs[ix][0]), device=device, dtype=torch.long), \\\n",
    "               torch.tensor(self.output_lang.indexesFromSentence(self.pairs[ix][1]), device=device, dtype=torch.long)\n",
    "    \n",
    "    def collate(self, batch):\n",
    "        # calcular longitud máxima en el batch \n",
    "        max_input_len, max_output_len = 0, 0\n",
    "        for input_sentence, output_sentence in batch:\n",
    "            max_input_len = len(input_sentence) if len(input_sentence) > max_input_len else max_input_len        \n",
    "            max_output_len = len(output_sentence) if len(output_sentence) > max_output_len else max_output_len\n",
    "        # añadimos padding a las frases cortas para que todas tengan la misma longitud\n",
    "        input_sentences, output_sentences = [], []\n",
    "        for input_sentence, output_sentence in batch:\n",
    "            input_sentences.append(torch.nn.functional.pad(input_sentence, (0, max_input_len - len(input_sentence)), 'constant', self.input_lang.word2index['PAD']))\n",
    "            output_sentences.append(torch.nn.functional.pad(output_sentence, (0, max_output_len - len(output_sentence)), 'constant', self.output_lang.word2index['PAD']))\n",
    "        # opcionalmente, podríamos re-ordenar las frases en el batch (algunos modelos lo requieren)\n",
    "        return torch.stack(input_sentences), torch.stack(output_sentences)\n",
    "\n",
    "# separamos datos en train-test\n",
    "train_size = len(pairs) * 80 // 100 \n",
    "train = pairs[:train_size]\n",
    "test = pairs[train_size:]\n",
    "\n",
    "dataset = {\n",
    "    'train': Dataset(input_lang, output_lang, train),\n",
    "    'test': Dataset(input_lang, output_lang, test)\n",
    "}\n",
    "\n",
    "len(dataset['train']), len(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:48:46.664097Z",
     "start_time": "2020-09-04T09:48:46.637664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3, 4, 1], device='cuda:0'), tensor([5, 4, 1], device='cuda:0'))"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence, output_sentence = dataset['train'][1]\n",
    "\n",
    "input_sentence, output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:48:46.771037Z",
     "start_time": "2020-09-04T09:48:46.765038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['go', '.', 'EOS'], ['vete', '.', 'EOS'])"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.sentenceFromIndex(input_sentence.tolist()), output_lang.sentenceFromIndex(output_sentence.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:48:46.912721Z",
     "start_time": "2020-09-04T09:48:46.890023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 11]), torch.Size([64, 13]))"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = {\n",
    "    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=64, shuffle=True, collate_fn=dataset['train'].collate),\n",
    "    'test': torch.utils.data.DataLoader(dataset['test'], batch_size=256, shuffle=False, collate_fn=dataset['test'].collate),\n",
    "}\n",
    "\n",
    "inputs, outputs = next(iter(dataloader['train']))\n",
    "inputs.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T08:13:53.670033Z",
     "start_time": "2020-09-04T08:13:53.652976Z"
    }
   },
   "source": [
    "## El modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos nuestros *dataloaders* listos, vamos a ver cómo construir nuestro modelo siguiendo la arquitectura `encoder-decoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El *encoder*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como `encoder` utilizaremos una `red neuronal recurrente` como las que ya hemos utilizado en los posts anteriores. Tendremos una primera capa `embedding` que se encargará de proveer a la `RNN` de la representación vectorial de cada palabra y luego la capa `RNN` (que puede ser también una `LSTM` o `GRU` como ya vimos en este [post](https://sensioai.com/blog/036_rnn_mejoras)). El `encoder` codificará la frase original y nos quedaremos con las salidas de las capas ocultas en el último paso (cuando ya ha visto toda la frase). Este tensor será el responsable de codificar el significado de la frase para que luego el `decoder` pueda traducirla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:48:47.865383Z",
     "start_time": "2020-09-04T09:48:47.848732Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=100, hidden_size=100, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=n_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, input_sentences):\n",
    "        embedded = self.embedding(input_sentences)\n",
    "        output, hidden = self.gru(embedded)\n",
    "        # del encoder nos interesa el último *hidden state* \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:48:48.285195Z",
     "start_time": "2020-09-04T09:48:48.252426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 100])"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(input_size=input_lang.n_words)\n",
    "hidden = encoder(torch.randint(0, input_lang.n_words, (64, 10)))\n",
    "\n",
    "# [num layers, batch size, hidden size]\n",
    "hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El *decoder*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El `decoder` será de nuevo una `red neuronal recurrente`. A diferencia del `encoder`, el estado oculto del `decoder` lo inicializaremos con el tensor obtenido a la salida del `encoder`. Tanto durante el entrenamiento como en fase de inferencia, le daremos al decoder como primera palabra el token `<sos>`. Con esta información, y el estado oculto del `encoder`, deberá predecir la primera palabra de la frase traducida. Seguidamente, usaremos esta primera palabra como nueva entrada junto al estado oculto obtenido en el paso anterior para generar la segunda palabra. Repetiremos este proceso hasta que el `decoder` nos de el token `<eos>`, indicando que la frase ha terminado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:48:48.658672Z",
     "start_time": "2020-09-04T09:48:48.647672Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=100, hidden_size=100, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.out = torch.nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, input_words, hidden):\n",
    "        embedded = self.embedding(input_words)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.out(output.squeeze(1))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:48:48.814724Z",
     "start_time": "2020-09-04T09:48:48.775932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 25274])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(input_size=output_lang.n_words)\n",
    "output, decoder_hidden = decoder(torch.randint(0, output_lang.n_words, (64, 1)), hidden)\n",
    "\n",
    "# [batch size, vocab size]\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:48:48.908518Z",
     "start_time": "2020-09-04T09:48:48.899724Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 100])"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [num layers, batch size, hidden size]\n",
    "decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a implementar el bucle de entrenamiento. En primer lugar, al tener ahora dos redes neuronales, necesitaremos dos optimizadores (uno para el `encoder` y otro para el `decoder`). Al `encoder` le pasaremos la frase en el idioma original, y obtendremos el estado oculto final. Este estado oculto lo usaremos para inicializar el `decoder` que, junto al token `<sos>`, generará la primera palabra de la frase traducida. Repetiremos el proceso, utilizando como entrada la anterior salida del decoder, hasta obtener el token `<eos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T09:48:49.887911Z",
     "start_time": "2020-09-04T09:48:49.860897Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def fit(encoder, decoder, dataloader, epochs=10):\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        train_loss = []\n",
    "        bar = tqdm(dataloader['train'])\n",
    "        for batch in bar:\n",
    "            input_sentences, output_sentences = batch\n",
    "            bs = input_sentences.shape[0]                    \n",
    "            loss = 0\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            # obtenemos el último estado oculto del encoder\n",
    "            hidden = encoder(input_sentences)\n",
    "            # calculamos las salidas del decoder de manera recurrente\n",
    "            decoder_input = torch.tensor([[output_lang.word2index['SOS']] for b in range(bs)], device=device)\n",
    "            for i in range(output_sentences.shape[1]):\n",
    "                output, hidden = decoder(decoder_input, hidden)\n",
    "                loss += criterion(output, output_sentences[:, i].view(bs))     \n",
    "                # el siguiente input será la palbra predicha\n",
    "                decoder_input = torch.argmax(output, axis=1).view(bs, 1)\n",
    "            # optimización\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            bar.set_description(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f}\")\n",
    "                    \n",
    "        val_loss = []\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            bar = tqdm(dataloader['test'])\n",
    "            for batch in bar:\n",
    "                input_sentences, output_sentences = batch\n",
    "                bs = input_sentences.shape[0]  \n",
    "                loss = 0\n",
    "                # obtenemos el último estado oculto del encoder\n",
    "                hidden = encoder(input_sentences)\n",
    "                # calculamos las salidas del decoder de manera recurrente\n",
    "                decoder_input = torch.tensor([[output_lang.word2index['SOS']] for b in range(bs)], device=device)\n",
    "                for i in range(output_sentences.shape[1]):\n",
    "                    output, hidden = decoder(decoder_input, hidden)\n",
    "                    loss += criterion(output, output_sentences[:, i].view(bs))     \n",
    "                    # el siguiente input será la palbra predicha\n",
    "                    decoder_input = torch.argmax(output, axis=1).view(bs, 1)\n",
    "                val_loss.append(loss.item())\n",
    "                bar.set_description(f\"Epoch {epoch}/{epochs} val_loss {np.mean(val_loss):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T10:16:18.499856Z",
     "start_time": "2020-09-04T10:04:26.283387Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 loss 17.94331: 100%|██████████████| 1557/1557 [01:04<00:00, 24.07it/s]\n",
      "Epoch 1/10 val_loss 63.72230: 100%|██████████████| 98/98 [00:06<00:00, 15.16it/s]\n",
      "Epoch 2/10 loss 16.96550: 100%|██████████████| 1557/1557 [01:04<00:00, 24.11it/s]\n",
      "Epoch 2/10 val_loss 64.63895: 100%|██████████████| 98/98 [00:06<00:00, 15.12it/s]\n",
      "Epoch 3/10 loss 16.16891: 100%|██████████████| 1557/1557 [01:04<00:00, 24.05it/s]\n",
      "Epoch 3/10 val_loss 64.69322: 100%|██████████████| 98/98 [00:06<00:00, 14.91it/s]\n",
      "Epoch 4/10 loss 15.51128: 100%|██████████████| 1557/1557 [01:04<00:00, 24.09it/s]\n",
      "Epoch 4/10 val_loss 64.71278: 100%|██████████████| 98/98 [00:06<00:00, 15.15it/s]\n",
      "Epoch 5/10 loss 14.93113: 100%|██████████████| 1557/1557 [01:04<00:00, 24.07it/s]\n",
      "Epoch 5/10 val_loss 65.13140: 100%|██████████████| 98/98 [00:06<00:00, 15.08it/s]\n",
      "Epoch 6/10 loss 14.42032: 100%|██████████████| 1557/1557 [01:04<00:00, 24.04it/s]\n",
      "Epoch 6/10 val_loss 65.81099: 100%|██████████████| 98/98 [00:06<00:00, 14.99it/s]\n",
      "Epoch 7/10 loss 13.97872: 100%|██████████████| 1557/1557 [01:04<00:00, 24.08it/s]\n",
      "Epoch 7/10 val_loss 66.21724: 100%|██████████████| 98/98 [00:06<00:00, 15.01it/s]\n",
      "Epoch 8/10 loss 13.57627: 100%|██████████████| 1557/1557 [01:04<00:00, 23.98it/s]\n",
      "Epoch 8/10 val_loss 67.11849: 100%|██████████████| 98/98 [00:06<00:00, 15.02it/s]\n",
      "Epoch 9/10 loss 13.20378: 100%|██████████████| 1557/1557 [01:04<00:00, 24.07it/s]\n",
      "Epoch 9/10 val_loss 66.49232: 100%|██████████████| 98/98 [00:06<00:00, 15.18it/s]\n",
      "Epoch 10/10 loss 12.89957: 100%|█████████████| 1557/1557 [01:04<00:00, 24.04it/s]\n",
      "Epoch 10/10 val_loss 68.54083: 100%|█████████████| 98/98 [00:06<00:00, 15.15it/s]\n"
     ]
    }
   ],
   "source": [
    "fit(encoder, decoder, dataloader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes ver, la *loss* de enterenamiento baja indicando que nuestra red está aprendiendo a traducir. Sin embargo, la *loss* de validación sube indicando que estamos haciendo *overfitting*. Esto es normal ya que estamos utilizando muy pocos datos para el entrenamiento, para reducir este problema tendrías que utilizar un dataset con más ejemplos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando traducciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos nuestro modelo entrenado, podemos utilizarlo para traducir frases del inglés al castellano de la siguiente manera. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T10:25:25.106385Z",
     "start_time": "2020-09-04T10:25:25.095709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['tom', 'taught', 'me', 'that', '.', 'EOS'],\n",
       " ['tom', 'me', 'enseno', 'eso', '.', 'EOS'])"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence, output_sentence = dataset['train'][19268]\n",
    "input_lang.sentenceFromIndex(input_sentence.tolist()), output_lang.sentenceFromIndex(output_sentence.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T10:25:25.217431Z",
     "start_time": "2020-09-04T10:25:25.205431Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(input_sentence):\n",
    "    # obtenemos el último estado oculto del encoder\n",
    "    hidden = encoder(input_sentence.unsqueeze(0))\n",
    "    # calculamos las salidas del decoder de manera recurrente\n",
    "    decoder_input = torch.tensor([[output_lang.word2index['SOS']]], device=device)\n",
    "    # iteramos hasta que el decoder nos de el token <eos>\n",
    "    outputs = []\n",
    "    while True:\n",
    "        output, hidden = decoder(decoder_input, hidden)\n",
    "        decoder_input = torch.argmax(output, axis=1).view(1, 1)\n",
    "        outputs.append(decoder_input.cpu().item())\n",
    "        if decoder_input.item() == output_lang.word2index['EOS']:\n",
    "            break\n",
    "    return output_lang.sentenceFromIndex(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T10:25:25.359588Z",
     "start_time": "2020-09-04T10:25:25.337923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tom', 'me', 'enseno', 'eso', '.', 'EOS']"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(input_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos aprendido a implementar una arquitectura `encoder-decoder` que nos permite trabajar con secuencia de longitud arbitraria tanto en las entradas como en las salida. El ejemplo de aplicación que hemos llevado a cabo es la traducción de texto. Esta arquitectura es muy versátil y puede utilizarse, con pequeños cambios, para otras aplicaciones como generación de descripciones a partir de imágenes (cambiando el encoder por una red convolucional, por ejemplo). Si bien esta arquitectura nos permite obtener buenos resultados, se ve limitada en el caso en el que trabajemos con secuencias muy largas, ya que el último estado del `encoder` es responsable de codificar todo el significado de la frase original, lo cual puede ser difícil. Podemos mejorar esta arquitectura añadiendo una capa de `atenttion` en el `decoder`, el cual no solo recibirá este estado oculto del `encoder` si no que además será capaz de mirar a todas las salidas del mismo para decidir, en cada caso, la mejor palabra a traducir. En el próximo post veremos como implementar este nuevo mecanismo."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "nlp_transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3ddabfba841642efa8c92aa0fa4cabf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "482d8ecf35a44521945b1f760ee31021": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6019610946264ce1a7a2e79cfcd82fa5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fbe31372cd24cdd9cf2ac44ad3067e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e370cbea737a47e8bb35feba70b04cb8",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ddabfba841642efa8c92aa0fa4cabf8",
      "value": 231508
     }
    },
    "922443b940dc4237b7d3bd7efff5ea3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6019610946264ce1a7a2e79cfcd82fa5",
      "placeholder": "​",
      "style": "IPY_MODEL_c79bf36cd9b34317a6b6c5905ab362bd",
      "value": " 232k/232k [00:30&lt;00:00, 7.64kB/s]"
     }
    },
    "c79bf36cd9b34317a6b6c5905ab362bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e370cbea737a47e8bb35feba70b04cb8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efedcba8722e430aa42a49fadd498011": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6fbe31372cd24cdd9cf2ac44ad3067e8",
       "IPY_MODEL_922443b940dc4237b7d3bd7efff5ea3a"
      ],
      "layout": "IPY_MODEL_482d8ecf35a44521945b1f760ee31021"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
