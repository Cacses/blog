{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yg90deR13qYE"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/041_attention/attention.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Kzp2jHl3qY7"
   },
   "source": [
    "# Mecanismos de Atención"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el [post](https://sensioai.com/blog/040_encoder_decoder) anterior aprendimos a implementar una arquitectura de red neuronal conocida como `seq2seq`, que utiliza dos redes neuronales (el `encoder` y el `decoder`) para poder trabajar con secuencias de longitud arbitraria tanto a sus entradas como en las salidas. Este modelo nos permite llevar a cabo tareas tales como la traducción de texto entre dos idiomas, resumir un texto, responder preguntas, etc.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
    "\n",
    "Si bien este modelo nos dio buenos resultados, podemos mejorarlo. Si prestamos atención a la arquitectura que desarrollamos, el `decoder` (encargado de generar la secuencia de salida) es inicializado con el último estado oculto del `encoder`, el cual tiene la responsabilidad de codificar el significado de toda la frase original. Esto puede ser complicado, sobre todo al trabajar con secuencias muy largas, y para solventar este problema podemos utilizar un mecanismo de `atención` que no solo reciba el último estado oculto si no también tenga acceso a todas las salidas del `encoder` de manera que el `decoder` sea capaz de \"focalizar su atención\" en aquellas partes más importantes. Por ejemplo, para traducir la primera palabra es lógico pensar que lo más importante será la primera palabra y sus adyacentes en la frase original, pero usar el último estado oculto del `encoder` puede no ser suficiente para mantener estas relaciones a largo plazo. Permitir al `decoder` acceder a esta información puede resultar en mejores prestaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 💡 En la práctica, los mecanismos de atención dan muy buenos resultados en tareas que envuelvan datos secuenciales (como aplicaciones de lenguaje). De hecho, los mejores modelos a día de hoy para tareas de `NLP` no están basados en redes recurrentes sino en arquitecturas que únicamente implementan mecanismos de atención en varias capas. Estas redes neuronales son conocidas como `Transformers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El *dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-03T08:31:48.390280Z",
     "start_time": "2020-09-03T08:31:48.382280Z"
    }
   },
   "source": [
    "Vamos a resolver exactamente el mismo caso que en el post anterior, así que todo lo que hace referencia al procesado de datos lo dejaremos igual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:28.282592Z",
     "start_time": "2020-09-04T14:01:28.264587Z"
    }
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def read_file(file, reverse=False):\n",
    "    # Read the file and split into lines\n",
    "    lines = open(file, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:31.458586Z",
     "start_time": "2020-09-04T14:01:28.284587Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs = read_file('spa.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:31.474120Z",
     "start_time": "2020-09-04T14:01:31.459587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['why wait until monday ?', ' por que esperar al lunes ?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:31.490116Z",
     "start_time": "2020-09-04T14:01:31.477120Z"
    }
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"SOS\": 0, \"EOS\": 1, \"PAD\": 2}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
    "        self.n_words = 3  # Count SOS, EOS and PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def indexesFromSentence(self, sentence):\n",
    "        return [self.word2index[word] for word in sentence.split(' ')]\n",
    "    \n",
    "    def sentenceFromIndex(self, index):\n",
    "        return [self.index2word[ix] for ix in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder aplicar la capa de `attention` necesitamos que nuestras frases tengan una longitud máxima definida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:31.506115Z",
     "start_time": "2020-09-04T14:01:31.491117Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPairs(pairs, filters, lang=0):\n",
    "    return [p for p in pairs if p[lang].startswith(filters)]\n",
    "\n",
    "def trimPairs(pairs):\n",
    "    return [p for p in pairs if len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:35.407683Z",
     "start_time": "2020-09-04T14:01:31.507116Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos 124547 pares de frases\n",
      "Tenemos 124124 pares de frases con longitud menor de 20\n",
      "Longitud vocabularios:\n",
      "spa 12948\n",
      "eng 25067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['can you speak french ? EOS', ' hablas frances ? EOS']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepareData(file, filters=None, reverse=False):\n",
    "    \n",
    "    pairs = read_file(file, reverse)\n",
    "    print(f\"Tenemos {len(pairs)} pares de frases\")\n",
    "    \n",
    "    if filters is not None:\n",
    "        pairs = filterPairs(pairs, filters, int(reverse))\n",
    "        print(f\"Filtramos a {len(pairs)} pares de frases\")\n",
    "        \n",
    "    pairs = trimPairs(pairs)\n",
    "    print(f\"Tenemos {len(pairs)} pares de frases con longitud menor de {MAX_LENGTH}\")\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang('eng')\n",
    "        output_lang = Lang('spa')\n",
    "    else:\n",
    "        input_lang = Lang('spa')\n",
    "        output_lang = Lang('eng')\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "        \n",
    "        # add <eos> token\n",
    "        pair[0] += \" EOS\"\n",
    "        pair[1] += \" EOS\"\n",
    "                           \n",
    "    print(\"Longitud vocabularios:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "                           \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('spa.txt')\n",
    "\n",
    "# descomentar para usar el dataset filtrado\n",
    "#input_lang, output_lang, pairs = prepareData('spa.txt', filters=eng_prefixes)\n",
    "                           \n",
    "random.choice(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:35.422738Z",
     "start_time": "2020-09-04T14:01:35.409686Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[204, 102, 4808, 4, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.indexesFromSentence('soy de francia . EOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:35.438737Z",
     "start_time": "2020-09-04T14:01:35.423740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hola', 'llore', 'pruebalo', 'vete', 'EOS']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lang.sentenceFromIndex([8, 91, 120, 5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el `Dataset` nos aseguraremos de añadir el *padding* necesario para que todas las frases tengan la misma longitud, lo cual no hace necesario utilizar la función `collate` que implementamos en el post anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:35.911736Z",
     "start_time": "2020-09-04T14:01:35.439737Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99299, 24825)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_lang, output_lang, pairs, max_length):\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "        self.pairs = pairs\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "        \n",
    "    def __getitem__(self, ix):        \n",
    "        inputs = torch.tensor(self.input_lang.indexesFromSentence(self.pairs[ix][0]), device=device, dtype=torch.long)\n",
    "        outputs = torch.tensor(self.output_lang.indexesFromSentence(self.pairs[ix][1]), device=device, dtype=torch.long)\n",
    "        # metemos padding a todas las frases hast a la longitud máxima\n",
    "        return torch.nn.functional.pad(inputs, (0, self.max_length - len(inputs)), 'constant', self.input_lang.word2index['PAD']), \\\n",
    "            torch.nn.functional.pad(outputs, (0, self.max_length - len(outputs)), 'constant', self.output_lang.word2index['PAD'])\n",
    "\n",
    "# separamos datos en train-test\n",
    "train_size = len(pairs) * 80 // 100 \n",
    "train = pairs[:train_size]\n",
    "test = pairs[train_size:]\n",
    "\n",
    "dataset = {\n",
    "    'train': Dataset(input_lang, output_lang, train, max_length=MAX_LENGTH),\n",
    "    'test': Dataset(input_lang, output_lang, test, max_length=MAX_LENGTH)\n",
    "}\n",
    "\n",
    "len(dataset['train']), len(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.039302Z",
     "start_time": "2020-09-04T14:01:35.912736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "        device='cuda:0'),\n",
       " tensor([5, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence, output_sentence = dataset['train'][1]\n",
    "\n",
    "input_sentence, output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.054828Z",
     "start_time": "2020-09-04T14:01:37.040302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['go',\n",
       "  '.',\n",
       "  'EOS',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD'],\n",
       " ['vete',\n",
       "  '.',\n",
       "  'EOS',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.sentenceFromIndex(input_sentence.tolist()), output_lang.sentenceFromIndex(output_sentence.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.086827Z",
     "start_time": "2020-09-04T14:01:37.055828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 20]), torch.Size([64, 20]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = {\n",
    "    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=64, shuffle=True),\n",
    "    'test': torch.utils.data.DataLoader(dataset['test'], batch_size=256, shuffle=False),\n",
    "}\n",
    "\n",
    "inputs, outputs = next(iter(dataloader['train']))\n",
    "inputs.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T08:13:53.670033Z",
     "start_time": "2020-09-04T08:13:53.652976Z"
    }
   },
   "source": [
    "## El modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lo que se refiere al `encoder`, seguimos usando exactamente la misma arquitectura. La única diferencia es que, además del último estado oculto, necesitaremos todas sus salidas para que el `decoder` pueda usarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.102827Z",
     "start_time": "2020-09-04T14:01:37.087831Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=100, hidden_size=100, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=n_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, input_sentences):\n",
    "        embedded = self.embedding(input_sentences)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.134828Z",
     "start_time": "2020-09-04T14:01:37.103832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20, 100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(input_size=input_lang.n_words)\n",
    "encoder_outputs, encoder_hidden = encoder(torch.randint(0, input_lang.n_words, (64, MAX_LENGTH)))\n",
    "\n",
    "# [batch size, seq len, hidden size]\n",
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.150827Z",
     "start_time": "2020-09-04T14:01:37.135832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [num layers, batch size, hidden size]\n",
    "encoder_hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El *decoder* con *attention*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver un ejemplo de implementación de una capa de atención para nuestro `decoder`. En primer lugar tendremos una capa lineal que recibirá como entradas los `embeddings` y el estado oculto anterior (concatenados). Esta capa lineal nos dará a la salida tantos valores como elementos tengamos en nuestras secuencias de entrada (recuerda que las hemos forzado a tener una longitud determinada). Después, aplicaremos una función `softmax` sobre estos valores obteniendo así una distribución de probabilidad que, seguidamente, multiplicaremos por los *outputs* del encoder (que también tienen la misma longitud). En esta función de probabilidad, cada elemento tiene un valor entre 0 y 1. Así pues, esta operación dará más importancia a aquellos *outputs* del `encoder` más importantes mientras que al resto les asignará unos valores cercanos a 0. A continuación, concatenaremos estos valores con los `embeddings`, de nuevo, y se lo daremos a una nueva capa lineal que combinará estos `embeddings` con los *outputs* del `encoder` re-escalados para obtener así los *inputs* finales de la capa recurrente. \n",
    "\n",
    "En resumen, usaremos las entradas y estado oculto del `decoder` para encontrar unos pesos que re-escalarán las salidas del `encoder`, los cuales combinaremos de nuevo con las entradas del `decoder` para obtener las representaciones finales de nuestras frases que alimentan la capa recurrente.\n",
    "\n",
    "![](https://i.imgur.com/1152PYf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.166827Z",
     "start_time": "2020-09-04T14:01:37.151827Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttnDecoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=100, hidden_size=100, n_layers=2, max_length=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.out = torch.nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "        # attention\n",
    "        self.attn = torch.nn.Linear(hidden_size + embedding_size, max_length)\n",
    "        self.attn_combine = torch.nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_words, hidden, encoder_outputs):\n",
    "        # sacamos los embeddings\n",
    "        embedded = self.embedding(input_words)\n",
    "        # calculamos los pesos de la capa de atención\n",
    "        attn_weights = torch.nn.functional.softmax(self.attn(torch.cat((embedded.squeeze(1), hidden[0]), dim=1))) \n",
    "        # re-escalamos los outputs del encoder con estos pesos\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "        output = torch.cat((embedded.squeeze(1), attn_applied.squeeze(1)), 1)\n",
    "        # aplicamos la capa de atención\n",
    "        output = self.attn_combine(output)\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        # a partir de aquí, como siempre. La diferencia es que la entrada a la RNN\n",
    "        # no es directmanete el embedding sino una combinación del embedding\n",
    "        # y las salidas del encoder re-escaladas\n",
    "        output, hidden = self.gru(output.unsqueeze(1), hidden)\n",
    "        output = self.out(output.squeeze(1))        \n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.214828Z",
     "start_time": "2020-09-04T14:01:37.167830Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sensio\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 25067])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = AttnDecoder(input_size=output_lang.n_words)\n",
    "decoder_output, decoder_hidden, attn_weights = decoder(torch.randint(0, output_lang.n_words, (64, 1)), encoder_hidden, encoder_outputs)\n",
    "\n",
    "# [batch size, vocab size]\n",
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.230826Z",
     "start_time": "2020-09-04T14:01:37.215829Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [num layers, batch size, hidden size]\n",
    "decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.246825Z",
     "start_time": "2020-09-04T14:01:37.231826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 100])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [num layers, batch size, hidden size]\n",
    "decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.262825Z",
     "start_time": "2020-09-04T14:01:37.247828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [batch size, max_length]\n",
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a implementar el bucle de entrenamiento. En primer lugar, al tener ahora dos redes neuronales, necesitaremos dos optimizadores (uno para el `encoder` y otro para el `decoder`). Al `encoder` le pasaremos la frase en el idioma original, y obtendremos el estado oculto final. Este estado oculto lo usaremos para inicializar el `decoder` que, junto al token `<sos>`, generará la primera palabra de la frase traducida. Repetiremos el proceso, utilizando como entrada la anterior salida del decoder, hasta obtener el token `<eos>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T14:01:37.278825Z",
     "start_time": "2020-09-04T14:01:37.263829Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def fit(encoder, decoder, dataloader, epochs=10):\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        train_loss = []\n",
    "        bar = tqdm(dataloader['train'])\n",
    "        for batch in bar:\n",
    "            input_sentences, output_sentences = batch\n",
    "            bs = input_sentences.shape[0]                    \n",
    "            loss = 0\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            # obtenemos el último estado oculto del encoder\n",
    "            encoder_outputs, hidden = encoder(input_sentences)\n",
    "            # calculamos las salidas del decoder de manera recurrente\n",
    "            decoder_input = torch.tensor([[output_lang.word2index['SOS']] for b in range(bs)], device=device)\n",
    "            for i in range(output_sentences.shape[1]):\n",
    "                output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n",
    "                loss += criterion(output, output_sentences[:, i].view(bs))     \n",
    "                # el siguiente input será la palabra predicha\n",
    "                decoder_input = torch.argmax(output, axis=1).view(bs, 1)\n",
    "            # optimización\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            bar.set_description(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f}\")\n",
    "            \n",
    "        val_loss = []\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            bar = tqdm(dataloader['test'])\n",
    "            for batch in bar:\n",
    "                input_sentences, output_sentences = batch\n",
    "                bs = input_sentences.shape[0]  \n",
    "                loss = 0\n",
    "                # obtenemos el último estado oculto del encoder\n",
    "                encoder_outputs, hidden = encoder(input_sentences)\n",
    "                # calculamos las salidas del decoder de manera recurrente\n",
    "                decoder_input = torch.tensor([[output_lang.word2index['SOS']] for b in range(bs)], device=device)\n",
    "                for i in range(output_sentences.shape[1]):\n",
    "                    output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n",
    "                    loss += criterion(output, output_sentences[:, i].view(bs))     \n",
    "                    # el siguiente input será la palabra predicha\n",
    "                    decoder_input = torch.argmax(output, axis=1).view(bs, 1)\n",
    "                val_loss.append(loss.item())\n",
    "                bar.set_description(f\"Epoch {epoch}/{epochs} val_loss {np.mean(val_loss):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T15:01:26.363992Z",
     "start_time": "2020-09-04T14:01:37.280826Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/1552 [00:00<?, ?it/s]C:\\Users\\sensio\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Epoch 1/30 loss 40.64597: 100%|██████████████| 1552/1552 [01:51<00:00, 13.88it/s]\n",
      "Epoch 1/30 val_loss 72.90058: 100%|██████████████| 97/97 [00:07<00:00, 13.20it/s]\n",
      "Epoch 2/30 loss 32.72146: 100%|██████████████| 1552/1552 [01:51<00:00, 13.90it/s]\n",
      "Epoch 2/30 val_loss 70.69164: 100%|██████████████| 97/97 [00:07<00:00, 13.02it/s]\n",
      "Epoch 3/30 loss 29.16241: 100%|██████████████| 1552/1552 [01:51<00:00, 13.90it/s]\n",
      "Epoch 3/30 val_loss 69.51955: 100%|██████████████| 97/97 [00:07<00:00, 13.28it/s]\n",
      "Epoch 4/30 loss 26.59795: 100%|██████████████| 1552/1552 [01:51<00:00, 13.94it/s]\n",
      "Epoch 4/30 val_loss 68.21252: 100%|██████████████| 97/97 [00:07<00:00, 13.27it/s]\n",
      "Epoch 5/30 loss 24.62131: 100%|██████████████| 1552/1552 [01:51<00:00, 13.91it/s]\n",
      "Epoch 5/30 val_loss 66.15660: 100%|██████████████| 97/97 [00:07<00:00, 13.26it/s]\n",
      "Epoch 6/30 loss 22.98447: 100%|██████████████| 1552/1552 [01:51<00:00, 13.89it/s]\n",
      "Epoch 6/30 val_loss 66.31125: 100%|██████████████| 97/97 [00:07<00:00, 13.20it/s]\n",
      "Epoch 7/30 loss 21.60687: 100%|██████████████| 1552/1552 [01:52<00:00, 13.78it/s]\n",
      "Epoch 7/30 val_loss 68.65877: 100%|██████████████| 97/97 [00:07<00:00, 13.10it/s]\n",
      "Epoch 8/30 loss 20.43874: 100%|██████████████| 1552/1552 [01:51<00:00, 13.96it/s]\n",
      "Epoch 8/30 val_loss 67.03853: 100%|██████████████| 97/97 [00:07<00:00, 13.37it/s]\n",
      "Epoch 9/30 loss 19.42336: 100%|██████████████| 1552/1552 [01:51<00:00, 13.94it/s]\n",
      "Epoch 9/30 val_loss 67.14857: 100%|██████████████| 97/97 [00:07<00:00, 13.65it/s]\n",
      "Epoch 10/30 loss 18.53758: 100%|█████████████| 1552/1552 [01:51<00:00, 13.92it/s]\n",
      "Epoch 10/30 val_loss 67.05894: 100%|█████████████| 97/97 [00:07<00:00, 13.00it/s]\n",
      "Epoch 11/30 loss 17.76296: 100%|█████████████| 1552/1552 [01:50<00:00, 13.98it/s]\n",
      "Epoch 11/30 val_loss 67.14232: 100%|█████████████| 97/97 [00:07<00:00, 13.47it/s]\n",
      "Epoch 12/30 loss 17.04589: 100%|█████████████| 1552/1552 [01:51<00:00, 13.96it/s]\n",
      "Epoch 12/30 val_loss 67.66175: 100%|█████████████| 97/97 [00:07<00:00, 13.33it/s]\n",
      "Epoch 13/30 loss 16.43274: 100%|█████████████| 1552/1552 [01:51<00:00, 13.88it/s]\n",
      "Epoch 13/30 val_loss 69.09210: 100%|█████████████| 97/97 [00:07<00:00, 13.23it/s]\n",
      "Epoch 14/30 loss 15.85514: 100%|█████████████| 1552/1552 [01:51<00:00, 13.92it/s]\n",
      "Epoch 14/30 val_loss 69.10287: 100%|█████████████| 97/97 [00:07<00:00, 13.53it/s]\n",
      "Epoch 15/30 loss 15.35156: 100%|█████████████| 1552/1552 [01:51<00:00, 13.87it/s]\n",
      "Epoch 15/30 val_loss 69.33862: 100%|█████████████| 97/97 [00:07<00:00, 13.09it/s]\n",
      "Epoch 16/30 loss 14.87437: 100%|█████████████| 1552/1552 [01:52<00:00, 13.77it/s]\n",
      "Epoch 16/30 val_loss 69.26039: 100%|█████████████| 97/97 [00:07<00:00, 13.11it/s]\n",
      "Epoch 17/30 loss 14.46223: 100%|█████████████| 1552/1552 [01:52<00:00, 13.77it/s]\n",
      "Epoch 17/30 val_loss 69.53232: 100%|█████████████| 97/97 [00:07<00:00, 13.09it/s]\n",
      "Epoch 18/30 loss 14.07377: 100%|█████████████| 1552/1552 [01:52<00:00, 13.77it/s]\n",
      "Epoch 18/30 val_loss 72.59765: 100%|█████████████| 97/97 [00:07<00:00, 13.09it/s]\n",
      "Epoch 19/30 loss 13.71036: 100%|█████████████| 1552/1552 [01:52<00:00, 13.75it/s]\n",
      "Epoch 19/30 val_loss 72.49108: 100%|█████████████| 97/97 [00:07<00:00, 13.13it/s]\n",
      "Epoch 20/30 loss 13.39023: 100%|█████████████| 1552/1552 [01:52<00:00, 13.76it/s]\n",
      "Epoch 20/30 val_loss 72.22702: 100%|█████████████| 97/97 [00:07<00:00, 13.15it/s]\n",
      "Epoch 21/30 loss 13.08446: 100%|█████████████| 1552/1552 [01:52<00:00, 13.75it/s]\n",
      "Epoch 21/30 val_loss 72.35934: 100%|█████████████| 97/97 [00:07<00:00, 13.02it/s]\n",
      "Epoch 22/30 loss 12.80340: 100%|█████████████| 1552/1552 [01:53<00:00, 13.73it/s]\n",
      "Epoch 22/30 val_loss 73.19710: 100%|█████████████| 97/97 [00:07<00:00, 13.15it/s]\n",
      "Epoch 23/30 loss 12.55116: 100%|█████████████| 1552/1552 [01:52<00:00, 13.75it/s]\n",
      "Epoch 23/30 val_loss 72.04197: 100%|█████████████| 97/97 [00:07<00:00, 13.05it/s]\n",
      "Epoch 24/30 loss 12.31468: 100%|█████████████| 1552/1552 [01:53<00:00, 13.73it/s]\n",
      "Epoch 24/30 val_loss 74.01761: 100%|█████████████| 97/97 [00:07<00:00, 13.08it/s]\n",
      "Epoch 25/30 loss 12.07109: 100%|█████████████| 1552/1552 [01:52<00:00, 13.74it/s]\n",
      "Epoch 25/30 val_loss 73.89849: 100%|█████████████| 97/97 [00:07<00:00, 13.07it/s]\n",
      "Epoch 26/30 loss 11.86944: 100%|█████████████| 1552/1552 [01:53<00:00, 13.72it/s]\n",
      "Epoch 26/30 val_loss 75.09626: 100%|█████████████| 97/97 [00:07<00:00, 13.13it/s]\n",
      "Epoch 27/30 loss 11.68174: 100%|█████████████| 1552/1552 [01:52<00:00, 13.74it/s]\n",
      "Epoch 27/30 val_loss 74.85488: 100%|█████████████| 97/97 [00:07<00:00, 13.07it/s]\n",
      "Epoch 28/30 loss 11.49805: 100%|█████████████| 1552/1552 [01:52<00:00, 13.73it/s]\n",
      "Epoch 28/30 val_loss 74.23841: 100%|█████████████| 97/97 [00:07<00:00, 13.00it/s]\n",
      "Epoch 29/30 loss 11.30296: 100%|█████████████| 1552/1552 [01:53<00:00, 13.73it/s]\n",
      "Epoch 29/30 val_loss 75.53842: 100%|█████████████| 97/97 [00:07<00:00, 13.04it/s]\n",
      "Epoch 30/30 loss 11.14230: 100%|█████████████| 1552/1552 [01:53<00:00, 13.73it/s]\n",
      "Epoch 30/30 val_loss 76.06929: 100%|█████████████| 97/97 [00:07<00:00, 13.10it/s]\n"
     ]
    }
   ],
   "source": [
    "fit(encoder, decoder, dataloader, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando traducciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos nuestro modelo entrenado, podemos utilizarlo para traducir frases del inglés al castellano de la siguiente manera. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T15:43:21.781076Z",
     "start_time": "2020-09-04T15:43:21.769077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['go',\n",
       "  '.',\n",
       "  'EOS',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD'],\n",
       " ['vete',\n",
       "  '.',\n",
       "  'EOS',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD',\n",
       "  'PAD'])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence, output_sentence = dataset['train'][1]\n",
    "input_lang.sentenceFromIndex(input_sentence.tolist()), output_lang.sentenceFromIndex(output_sentence.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T15:43:22.157946Z",
     "start_time": "2020-09-04T15:43:22.147947Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def predict(input_sentence):\n",
    "    # obtenemos el último estado oculto del encoder\n",
    "    encoder_outputs, hidden = encoder(input_sentence.unsqueeze(0))\n",
    "    # calculamos las salidas del decoder de manera recurrente\n",
    "    decoder_input = torch.tensor([[output_lang.word2index['SOS']]], device=device)\n",
    "    # iteramos hasta que el decoder nos de el token <eos>\n",
    "    outputs = []\n",
    "    decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n",
    "    i = 0\n",
    "    while True:\n",
    "        output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n",
    "        decoder_attentions[i] = attn_weights.data\n",
    "        i += 1\n",
    "        decoder_input = torch.argmax(output, axis=1).view(1, 1)\n",
    "        outputs.append(decoder_input.cpu().item())\n",
    "        if decoder_input.item() == output_lang.word2index['EOS']:\n",
    "            break\n",
    "    return output_lang.sentenceFromIndex(outputs), decoder_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T15:43:22.313634Z",
     "start_time": "2020-09-04T15:43:22.302633Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sensio\\miniconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ve', '.', 'EOS']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_words, attn = predict(input_sentence)\n",
    "output_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de atención"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las ventajas que nos da la capa de atención es que nos permite visualizar en qué partes de los inputs se fija el modelo para generar cada una de las palabras en el output, dando un grado de explicabilidad a nuestro modelo (una propiedad siempre deseada en nuestro modelos de `Machine Learning`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T15:43:23.404845Z",
     "start_time": "2020-09-04T15:43:23.397845Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    lim1, lim2 = input_sentence.index('EOS')+1, output_words.index('EOS')+1\n",
    "    fig = plt.figure(dpi=100)\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions[:lim2, :lim1].numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([' '] + input_sentence[:lim1], rotation=90)\n",
    "    ax.set_yticklabels([' '] + output_words)\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T15:43:23.762833Z",
     "start_time": "2020-09-04T15:43:23.633834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "showAttention(input_lang.sentenceFromIndex(input_sentence.tolist()), output_words, attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos visto como introducir mecanismos de atención en nuestra arquitectura `encoder-decoder`, los cuales permiten a nuestra red neuronal focalizarse en partes concretas de los *inputs* a la hora de generar los *outputs*. Esta nueva capa no solo puede mejorar nuestros modelos sino que además también es interpretable, dándonos una idea del razonamiento detrás de las predicciones de nuestro modelo. Las redes neuronales con mejores prestaciones a día de hoy en tareas de `NLP`, los `transformers`, están basados enteramente en este tipo de capas de atención. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "nlp_transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3ddabfba841642efa8c92aa0fa4cabf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "482d8ecf35a44521945b1f760ee31021": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6019610946264ce1a7a2e79cfcd82fa5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fbe31372cd24cdd9cf2ac44ad3067e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e370cbea737a47e8bb35feba70b04cb8",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ddabfba841642efa8c92aa0fa4cabf8",
      "value": 231508
     }
    },
    "922443b940dc4237b7d3bd7efff5ea3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6019610946264ce1a7a2e79cfcd82fa5",
      "placeholder": "​",
      "style": "IPY_MODEL_c79bf36cd9b34317a6b6c5905ab362bd",
      "value": " 232k/232k [00:30&lt;00:00, 7.64kB/s]"
     }
    },
    "c79bf36cd9b34317a6b6c5905ab362bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e370cbea737a47e8bb35feba70b04cb8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efedcba8722e430aa42a49fadd498011": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6fbe31372cd24cdd9cf2ac44ad3067e8",
       "IPY_MODEL_922443b940dc4237b7d3bd7efff5ea3a"
      ],
      "layout": "IPY_MODEL_482d8ecf35a44521945b1f760ee31021"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
