{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yg90deR13qYE"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/039_nlp_transfer/nlp_transfer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiDRydOc3qYF"
   },
   "source": [
    "# Clasificación de texto - Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TI2sYEw53qYG"
   },
   "source": [
    "En el [post](https://sensioai.com/blog/038_clasificacion_texto) anterior vimos cómo podemos entrenar una `red neuronal recurrente` para clasificar texto. En este tipo de tarea, nuestro modelo será capaz de asignar una etiqueta concreta entre varias a una pieza de texto determinada. Vimos, por ejemplo, que podemos saber de manera automática si una opinión de una película es positiva o negativa. En este post vamos a resolver exactamente el mismo caso, pero introduciendo una nueva técnica muy utilizada: el `transfer learning`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJjxLfcx3qYG"
   },
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mEuD8t93qYH"
   },
   "source": [
    "Esta técnica nos permite entrenar redes neuronales de manera más rápida, con menores requisitos computacionales y permitiendo el entrenamiento de redes con mejores prestaciones con pequeños datasets. La idea consiste en entrenar una red neuronal en un gran dataset, con grandes recursos computacionales, y una vez entrenada utilizar el conocimiento que este modelo ya posee como punto de partida para nuestro caso particular en el proceso conocido como `fine tuning`.\n",
    "\n",
    "![](https://pennylane.ai/qml/_images/transfer_learning_general.png)\n",
    "\n",
    "Este proceso de `fine tuning` puede variar según la tarea, pero lo más común es sustituir las capas finales de la red por nuevas capas adaptadas a nuestra tarea y entrenar solo estas nuevas capas, dejando intactas las capas ya entrenadas. Sin embargo, en el caso en el que los datos usados en la nueva tarea sean muy diferentes que los usados originalmente, también es común el entrenamiento de toda la red, a partir de los pesos pre-entrenados. \n",
    "\n",
    "Como comentábamos al principio, esta técnica es muy utilizada en la práctica. Podemos encontrar modelos pre-entrenados en diferentes librerías, que podemos descargar y empezar a utilizar directamente. El `transfer learning` es utilizado tanto en aplicaciones de lenguaje como tareas visuales, y lo usaremos de manera extensiva de ahora en adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YkprYd7j3qYH"
   },
   "source": [
    " ## El *dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l14aVDhe3qYH"
   },
   "source": [
    "Seguimos utilizando el dataset IMDB, disponible en [torchtext](https://pytorch.org/text/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:05:50.098816Z",
     "start_time": "2020-09-02T17:05:49.536511Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "XSI0LuJw3qYI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:06:49.385453Z",
     "start_time": "2020-09-02T17:05:50.099802Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "2HbseR3y3qYK",
    "outputId": "5b2617b9-6e90-4bcd-e151-cd45fef99c50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "TEXT = torchtext.data.Field(tokenize = 'spacy')\n",
    "LABEL = torchtext.data.LabelField(dtype = torch.long)\n",
    "\n",
    "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:06:49.401523Z",
     "start_time": "2020-09-02T17:06:49.386457Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "n-t5qGVr3qYN",
    "outputId": "a80707c1-46ea-4175-ac90-9db0e321f5a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:06:49.416536Z",
     "start_time": "2020-09-02T17:06:49.402524Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "8TWnH0iI3qYP",
    "outputId": "4dc37384-a44a-4dc7-c31f-1277de4b6362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['How', 'I', 'got', 'into', 'it', ':', 'When', 'I', 'started', 'watching', 'this', 'series', 'on', 'Cartoon', 'Network', ',', 'I', 'have', 'to', 'say', 'that', 'I', \"'ve\", 'never', 'seen', 'anything', 'like', 'this', ',', 'and', 'it', 'was', 'the', 'best', '.', 'But', 'when', 'I', 'started', 'collecting', 'the', 'series', 'on', 'VHS', ',', 'and', 'years', 'later', 'on', 'DVD', 'part', 'of', 'Bandai', \"'s\", 'Anime', 'Legends', 'collections', '.', 'It', 'was', 'amazing', ',', 'and', 'truly', 'worth', 'watching', '.', 'It', 'had', 'a', 'lot', 'of', 'exploding', 'action', 'that', 'will', 'blow', 'you', 'out', 'of', 'your', 'seat', '.', 'And', 'of', 'course', ',', 'the', 'theme', 'songs', '\"', 'Just', 'Communication\",and', 'Rhythm', 'Emotions', '\"', 'were', 'the', 'best.<br', '/><br', '/>Characters', ',', 'and', 'Gundams', ':', 'My', 'favorite', 'characters', 'in', 'the', 'show', 'were', ':', 'Heero', ',', 'Duo', ',', 'Relena', ',', 'Treize', ',', 'Lady', 'Und', ',', 'Noin', ',', 'and', 'Zechs', '.', 'My', 'favorite', 'Gundams', 'in', 'the', 'show', 'that', 'I', 'liked', 'the', 'most', 'are', 'the', 'Wing', 'Zero', ',', 'and', 'Epyon', ',', 'and', 'of', 'course', 'the', 'Altron', ',', 'and', 'Deathscythe', 'I', ',', 'and', 'II.<br', '/><br', '/>Meaning', 'of', 'the', 'show', ':', 'What', 'this', 'series', 'also', 'tells', 'us', 'that', 'in', 'real', 'life', ',', 'wars', 'are', 'very', 'hard', 'and', 'we', 'can', 'sometimes', 'win', ',', 'or', 'lose', '.', 'But', 'peace', 'can', 'also', 'be', 'hard', 'to', 'obtain', ',', 'and', 'I', 'do', 'believe', 'the', 'Gundam', 'pilots', 'are', 'doing', 'the', 'right', 'thing', ',', 'and', 'are', 'trying', 'to', 'obtain', 'world', 'peace.<br', '/><br', '/>But', 'however', ',', 'this', 'show', 'is', 'truly', 'the', 'best', 'of', 'the', 'best', '.', 'So', 'in', 'closing', 'to', 'this', 'review', ',', 'after', 'you', 'watch', 'this', 'show', ',', 'see', 'the', 'Movie', 'Endless', 'Waltz', '.'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qmURBdC33qYR"
   },
   "source": [
    "## *Embeddings* pre-entrenados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lujcFj4H3qYR"
   },
   "source": [
    "El primer ejemplo de `transfer learning` que vamos a ver es el uso de `embeddings` pre-entrenados. Recuerda que un embedding es la respresentación vectorial de cada palabra en el vocabulario que utilizaremos para alimentar nuestra red recurrente. Puedes aprender más sobre `embeddings` en este [post](https://sensioai.com/blog/037_charRNN). En `torchtext` podemos descargar estos `embeddings` en la función `build_vocab`, con el parámtero `vectors`. En la documentación encontrarás los diferentes `embeddings` disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:06:51.179497Z",
     "start_time": "2020-09-02T17:06:49.417539Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6N7gwB8F3qYS",
    "outputId": "850132d1-5cd1-4640-fc5b-4b3edac2bab7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 10000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", # embeddings pre-entrenados\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "len(TEXT.vocab), len(LABEL.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BHEbZ90n3qYW"
   },
   "source": [
    "Y, de la misma manera que hicimos en el post anterior, definimos nuestros *dataloaders* con la clase `torchtext.data.BucketIterator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:06:51.225297Z",
     "start_time": "2020-09-02T17:06:51.181498Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "adNca2SR3qYW",
    "outputId": "ddf9c460-70fa-4923-b831-e3e5aede036e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataloader = {\n",
    "    'train': torchtext.data.BucketIterator(train_data, batch_size=64, shuffle=True, sort_within_batch=True, device=device),\n",
    "    'test': torchtext.data.BucketIterator(test_data, batch_size=64, device=device)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PW7QtEu33qYZ"
   },
   "source": [
    "## El modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "psaait293qYZ"
   },
   "source": [
    "Usaremos exactamente el mismo modelo que ya vimos en el post anterior. Este modelo está compuesto, principalmente, por la capa `embedding`, que en este caso sustituiremos por los vectores descargados anteriormente, y las capas recurrente y lineal, que entrenaremos desde cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:06:51.240470Z",
     "start_time": "2020-09-02T17:06:51.226301Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "bnAt6VZJ3qYZ"
   },
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=128, hidden_dim=128, output_dim=2, num_layers=2, dropout=0.2, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(2*hidden_dim if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # no entrenamos los embeddings\n",
    "        with torch.no_grad():\n",
    "            #text = [sent len, batch size]        \n",
    "            embedded = self.embedding(text)        \n",
    "        #embedded = [sent len, batch size, emb dim]        \n",
    "        output, hidden = self.rnn(embedded)        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        y = self.fc(output[-1,:,:].squeeze(0))     \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-aP8aVb3qYb"
   },
   "source": [
    "Una vez definido el modelo, sustituimos los tensores en la capa `embedding` por los vectores pre-entrenados descargados anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:06:51.474095Z",
     "start_time": "2020-09-02T17:06:51.242472Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W0oSPUdI3qYb",
    "outputId": "508a9376-119a-433b-f3ac-f9cd1cdff645"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN(input_dim=len(TEXT.vocab), bidirectional=True, embedding_dim=100)\n",
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "# ponemos a cero los pesos correspondientes a los tokens <unk> y <pad>\n",
    "model.embedding.weight.data[TEXT.vocab.stoi[TEXT.unk_token]] = torch.zeros(100)\n",
    "model.embedding.weight.data[TEXT.vocab.stoi[TEXT.pad_token]] = torch.zeros(100)\n",
    "\n",
    "outputs = model(torch.randint(0, len(TEXT.vocab), (100, 64)))\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6bmOZwA3qYd"
   },
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vz6TyCjv3qYd"
   },
   "source": [
    "Para entrenar nuestra red usamos el bucle estándar que ya usamos en posts anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:06:51.489416Z",
     "start_time": "2020-09-02T17:06:51.476095Z"
    },
    "code_folding": [
     3
    ],
    "colab": {},
    "colab_type": "code",
    "id": "dixF27i_3qYe"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def fit(model, dataloader, epochs=5):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_loss, train_acc = [], []\n",
    "        bar = tqdm(dataloader['train'])\n",
    "        for batch in bar:\n",
    "            X, y = batch\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
    "            train_acc.append(acc)\n",
    "            bar.set_description(f\"loss {np.mean(train_loss):.5f} acc {np.mean(train_acc):.5f}\")\n",
    "        bar = tqdm(dataloader['test'])\n",
    "        val_loss, val_acc = [], []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in bar:\n",
    "                X, y = batch\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                y_hat = model(X)\n",
    "                loss = criterion(y_hat, y)\n",
    "                val_loss.append(loss.item())\n",
    "                acc = (y == torch.argmax(y_hat, axis=1)).sum().item() / len(y)\n",
    "                val_acc.append(acc)\n",
    "                bar.set_description(f\"val_loss {np.mean(val_loss):.5f} val_acc {np.mean(val_acc):.5f}\")\n",
    "        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f} acc {np.mean(train_acc):.5f} val_acc {np.mean(val_acc):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-02T17:01:41.410Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "6nPMXQub3qYg",
    "outputId": "6dd28e1a-b2f2-49ce-b0f8-59aeb6edaf00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "loss 0.60385 acc 0.65508: 100%|██████████| 391/391 [00:29<00:00, 13.47it/s]\n",
      "val_loss 1.20899 val_acc 0.50868: 100%|██████████| 391/391 [00:28<00:00, 13.63it/s]\n",
      "loss 0.41505 acc 0.85156:   0%|          | 1/391 [00:00<00:42,  9.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss 0.60385 val_loss 1.20899 acc 0.65508 val_acc 0.50868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.38606 acc 0.82816: 100%|██████████| 391/391 [00:28<00:00, 13.50it/s]\n",
      "val_loss 0.36242 val_acc 0.83732: 100%|██████████| 391/391 [00:28<00:00, 13.79it/s]\n",
      "loss 0.29536 acc 0.88281:   0%|          | 1/391 [00:00<00:46,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 loss 0.38606 val_loss 0.36242 acc 0.82816 val_acc 0.83732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.31376 acc 0.86516: 100%|██████████| 391/391 [00:29<00:00, 13.48it/s]\n",
      "val_loss 0.37173 val_acc 0.83473: 100%|██████████| 391/391 [00:29<00:00, 13.44it/s]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 loss 0.31376 val_loss 0.37173 acc 0.86516 val_acc 0.83473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.27493 acc 0.88288: 100%|██████████| 391/391 [00:29<00:00, 13.42it/s]\n",
      "val_loss 0.32504 val_acc 0.86353: 100%|██████████| 391/391 [00:28<00:00, 13.71it/s]\n",
      "loss 0.23483 acc 0.89062:   1%|          | 2/391 [00:00<00:31, 12.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 loss 0.27493 val_loss 0.32504 acc 0.88288 val_acc 0.86353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.23740 acc 0.90324: 100%|██████████| 391/391 [00:28<00:00, 13.50it/s]\n",
      "val_loss 0.31196 val_acc 0.86348: 100%|██████████| 391/391 [00:28<00:00, 13.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 loss 0.23740 val_loss 0.31196 acc 0.90324 val_acc 0.86348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLDcflUq3qYi"
   },
   "source": [
    "## Generando predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TV4ph12F3qYi"
   },
   "source": [
    "Una vez nuestro modelo ha sido entrenado, podemos generar predicciones exactamente igual que hicimos en el post anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:36:23.366252Z",
     "start_time": "2020-09-01T16:36:22.942250Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "WrocF-sx3qYj"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict(model, X):\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        X = torch.tensor(X).to(device)\n",
    "        pred = model(X)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T16:36:23.382248Z",
     "start_time": "2020-09-01T16:36:23.367255Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "1Z5iW-qc3qYk",
    "outputId": "d90baf65-8eba-4ab8-cf73-085f56a618da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"this film is terrible\", \"this film is great\", \"this film is good\", \"a waste of time\"]\n",
    "tokenized = [[tok.text for tok in nlp.tokenizer(sentence)] for sentence in sentences]\n",
    "indexed = [[TEXT.vocab.stoi[_t] for _t in t] for t in tokenized]\n",
    "tensor = torch.tensor(indexed).permute(1,0)\n",
    "predictions = torch.argmax(predict(model, tensor), axis=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2RrkI-yW3qYn"
   },
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtfVMXMj3qYn"
   },
   "source": [
    "Como has visto en el ejemplo anterior, utilizar unos `embeddings` pre-entrenados puede darnos mucho mejores resultados que entrenarlos desde cero, ya que la representación de nuestras palabras será mucho mejor desde el principio. Siguiendo en esta línea, podemos sustituir nuestra capa `embedding` por otro modelo que nos aportará todavía mejores resultados, un `transformer`. \n",
    "\n",
    "Estos modelos aparecieron alrededor de 2017, y fueron presentados en el famoso artículo [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf). Desde su aparación, estos modelos están batiendo todos los *benchmarks* en las diferentes tareas de procesado de lenguaje, y son utilizados como base de cualquier modelo competente a día de hoy. De momento, no entraremos en detalles en la definición de esta arquitectura (lo dejamos para un futuro post, ya que hay mucha tela que cortar) pero vamos a ver como utilizar un `transformer` para hacer `transfer learning` y obtener muy buenos resultados de manera rápida. \n",
    "\n",
    "Una librería muy utilizada para trabajar con estos modelos es la librería `transformers` de [huggingface](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:06:53.519427Z",
     "start_time": "2020-09-02T17:06:51.491415Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "4-98jW6m3qYn",
    "outputId": "8b6db352-6591-4929-9af4-ae02751b79d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 4.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 13.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting tokenizers==0.8.1.rc2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 31.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=be4fc63ff310a9eec20308c3065a101044bf9db73cd3797cc81f81440fd50cc0\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.43 tokenizers-0.8.1rc2 transformers-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l799IUwE3qYp"
   },
   "source": [
    "En primer lugar, tendremos que utilizar el mismo `tokenizer` utilizado para entrenar el modelo original. En este caso usaremos la red conocida como `BERT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:06:55.976181Z",
     "start_time": "2020-09-02T17:06:53.520429Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "efedcba8722e430aa42a49fadd498011",
      "482d8ecf35a44521945b1f760ee31021",
      "6fbe31372cd24cdd9cf2ac44ad3067e8",
      "922443b940dc4237b7d3bd7efff5ea3a",
      "3ddabfba841642efa8c92aa0fa4cabf8",
      "e370cbea737a47e8bb35feba70b04cb8",
      "c79bf36cd9b34317a6b6c5905ab362bd",
      "6019610946264ce1a7a2e79cfcd82fa5"
     ]
    },
    "colab_type": "code",
    "id": "zbdlmJg73qYp",
    "outputId": "51b1cc07-ccca-4d97-a778-8f0d1bd2b3cd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efedcba8722e430aa42a49fadd498011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:06:55.992130Z",
     "start_time": "2020-09-02T17:06:55.977181Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wAZ1S1O03qYr",
    "outputId": "fca803fe-be19-432c-c4be-863eb294ab2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'how', 'are', 'you', '?']"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:06:56.007534Z",
     "start_time": "2020-09-02T17:06:55.993130Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r6T3jhIT3qYt",
    "outputId": "21c065af-dfa6-4c62-b452-d11b95a34e2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7592, 2088, 2129, 2024, 2017, 1029]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvqpnWsi3qYv"
   },
   "source": [
    "A diferencia de las `redes neuronales recurrentes`, los transformers trabajan con longitudes de secuencia fijas (no son modelos recurrentes). Es por este motivo que tenemos que asegurarnos que ninguna frase en el dataset tiene mayor longitud que la máxima permitida por `BERT`, que es de 512 palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:07:03.793315Z",
     "start_time": "2020-09-02T17:07:03.779499Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "pOF7qzbb3qYv"
   },
   "outputs": [],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M2O7x5VU3qYw"
   },
   "source": [
    "`torchtext` nos da la libertad de definir nuestros propios tokenizers, y podemos incluirlos de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:22:18.054844Z",
     "start_time": "2020-09-02T17:22:18.039843Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "MbtJ6RWx3qYx",
    "outputId": "aa23402f-9d51-49f5-85cf-9edf76998ad7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "TEXT = torchtext.data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = tokenizer.cls_token_id,\n",
    "                  eos_token = tokenizer.sep_token_id,\n",
    "                  pad_token = tokenizer.pad_token_id,\n",
    "                  unk_token = tokenizer.unk_token_id)\n",
    "\n",
    "LABEL = torchtext.data.LabelField(dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:25:34.082483Z",
     "start_time": "2020-09-02T17:22:18.056843Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "PtjlybRJ3qYy",
    "outputId": "97895fa3-e953-4b3d-d02b-54c72f558f5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "dataloader = {\n",
    "    'train': torchtext.data.BucketIterator(train_data, batch_size=64, shuffle=True, sort_within_batch=True, device=device),\n",
    "    'test': torchtext.data.BucketIterator(test_data, batch_size=64, device=device)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kg-BtVHe3qYz"
   },
   "source": [
    "Una vez tenemos los datos preparados con el nuevo tokenizer, necesitamos definir nuestro nuevo modelo. En este caso, `BERT` se encargará de actuar como nuestra capa `embedding`, proveyendo de la mejor representación posible de nuestro texto para que las siguientes capas puedan clasificarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T17:25:34.099485Z",
     "start_time": "2020-09-02T17:25:34.083484Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Po0hmYTQ3qY0"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class BERT(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim=256, output_dim=2, n_layers=2, bidirectional=True, dropout=0.2):\n",
    "        super().__init__()        \n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')        \n",
    "        \n",
    "        # freeze BERT\n",
    "        for name, param in self.bert.named_parameters():                \n",
    "            if name.startswith('bert'):\n",
    "                param.requires_grad = False\n",
    "\n",
    "        embedding_dim = self.bert.config.to_dict()['hidden_size']\n",
    "        self.rnn = torch.nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):                       \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "        output, hidden = self.rnn(embedded)        \n",
    "        y = self.fc(output[:,-1,:].squeeze(1))     \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-02T17:19:06.120Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "epXiRbi03qY3",
    "outputId": "eada64bb-b721-4f3d-ad37-9aada1538162"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "loss 0.35525 acc 0.83775: 100%|██████████| 391/391 [04:41<00:00,  1.39it/s]\n",
      "val_loss 0.24976 val_acc 0.90348: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss 0.35525 val_loss 0.24976 acc 0.83775 val_acc 0.90348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.22018 acc 0.91375: 100%|██████████| 391/391 [04:41<00:00,  1.39it/s]\n",
      "val_loss 0.29960 val_acc 0.86585: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 loss 0.22018 val_loss 0.29960 acc 0.91375 val_acc 0.86585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.18880 acc 0.92842: 100%|██████████| 391/391 [04:41<00:00,  1.39it/s]\n",
      "val_loss 0.44130 val_acc 0.77996: 100%|██████████| 391/391 [07:33<00:00,  1.16s/it]\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 loss 0.18880 val_loss 0.44130 acc 0.92842 val_acc 0.77996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss 0.17249 acc 0.92578:   2%|▏         | 8/391 [00:05<04:12,  1.52it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e7b600eb8914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-d449ae58387e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, dataloader, epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BERT()\n",
    "fit(model, dataloader, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_tLNWtFE4gC"
   },
   "source": [
    "Puedes ver que en la primera *epoch* ya tenemos un modelo mejor que cualquiera de los obtenidos anteriormente cuando entrenamos desde cero. Y finalmente, podemos generar las predicciones de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rnp0yU6O3qY5"
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    tokenized = [tok[:max_input_length-2] for tok in tokenizer.tokenize(sentence)]\n",
    "    indexed = [tokenizer.cls_token_id] + tokenizer.convert_tokens_to_ids(tokenized) + [tokenizer.sep_token_id]\n",
    "    tensor = torch.tensor([indexed]).to(device)\n",
    "    model.eval()\n",
    "    return torch.argmax(model(tensor), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4MEhc0rJG8j-",
    "outputId": "51936411-0115-4c4f-ecc8-07172bf46198"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1], device='cuda:0'), tensor([0], device='cuda:0')]"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"Best film ever !\", \"this movie is terrible\"]\n",
    "preds = [predict(s) for s in sentences]\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ql2zt14l3qY7"
   },
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Kzp2jHl3qY7"
   },
   "source": [
    "En este post hemos visto como podemos obtener mejores modelos de clasificación de texto si utilizamos el `transfer learning`. Con esta técnica, sustituiremos las primeras capas de nuestro modelo por otros modelos ya entrenados en otros datasets. Dadas las condiciones adecuadas, esta técnica nos va a permitir entrenar modelos muy buenos con pocos datos y en poco tiempo. En nuestro caso, hemos conseguido nuestro mejor clasificador utilizando el modelo `BERT` como capa `embedding` y entrenando nuestro modelo recurrente encima, el cual solo hemos necesitado entrenar por una epoch. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "nlp_transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3ddabfba841642efa8c92aa0fa4cabf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "482d8ecf35a44521945b1f760ee31021": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6019610946264ce1a7a2e79cfcd82fa5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fbe31372cd24cdd9cf2ac44ad3067e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e370cbea737a47e8bb35feba70b04cb8",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ddabfba841642efa8c92aa0fa4cabf8",
      "value": 231508
     }
    },
    "922443b940dc4237b7d3bd7efff5ea3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6019610946264ce1a7a2e79cfcd82fa5",
      "placeholder": "​",
      "style": "IPY_MODEL_c79bf36cd9b34317a6b6c5905ab362bd",
      "value": " 232k/232k [00:30&lt;00:00, 7.64kB/s]"
     }
    },
    "c79bf36cd9b34317a6b6c5905ab362bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e370cbea737a47e8bb35feba70b04cb8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efedcba8722e430aa42a49fadd498011": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6fbe31372cd24cdd9cf2ac44ad3067e8",
       "IPY_MODEL_922443b940dc4237b7d3bd7efff5ea3a"
      ],
      "layout": "IPY_MODEL_482d8ecf35a44521945b1f760ee31021"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
