{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/029_pytorch_datasets/pytorch_datasets.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch - Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los posts anteriores hemos introducido los conceptos fundamentales de la librería de `Deep Learning` `Pytorch` y también hemos visto la funcionalidad que nos ofrece a la hora de diseñar y entrenar `redes neuronales`. En este post nos enfocamos en la herramientas que la librería nos da a la hora definir nuestros *datasets*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T13:17:52.168031Z",
     "start_time": "2020-08-15T13:17:51.679881Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterando tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los posts anteriores hemos utilizado el dataset MNIST para ilustrar los diferentes ejemplos que hemos visto. Vamos a seguir con este caso. A continuación tenemos una implementación en la que iteramos por los datos de manera explícita para entrenar nuestra red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T13:18:12.317390Z",
     "start_time": "2020-08-15T13:17:52.169393Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "# descarga datos\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, Y = mnist[\"data\"], mnist[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:60000] / 255., X[60000:] / 255., Y[:60000].astype(np.int), Y[60000:].astype(np.int)\n",
    "\n",
    "X_t = torch.from_numpy(X_train).float().cuda()\n",
    "Y_t = torch.from_numpy(y_train).long().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T13:18:12.348390Z",
     "start_time": "2020-08-15T13:18:12.319392Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(axis=-1,keepdims=True)\n",
    "\n",
    "def evaluate(x):\n",
    "    model.eval()\n",
    "    y_pred = model(x)\n",
    "    y_probas = softmax(y_pred)\n",
    "    return torch.argmax(y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T13:18:13.829898Z",
     "start_time": "2020-08-15T13:18:12.349391Z"
    },
    "code_folding": [
     15
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 Loss 1.86759\n",
      "Epoch 20/100 Loss 1.50102\n",
      "Epoch 30/100 Loss 1.20334\n",
      "Epoch 40/100 Loss 1.02917\n",
      "Epoch 50/100 Loss 0.89791\n",
      "Epoch 60/100 Loss 0.80771\n",
      "Epoch 70/100 Loss 0.73555\n",
      "Epoch 80/100 Loss 0.67983\n",
      "Epoch 90/100 Loss 0.63525\n",
      "Epoch 100/100 Loss 0.59748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9319"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_in, H, D_out = 784, 100, 10\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ").to(\"cuda\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.8)\n",
    "\n",
    "epochs = 100\n",
    "log_each = 10\n",
    "l = []\n",
    "model.train()\n",
    "for e in range(1, epochs+1): \n",
    "    \n",
    "    # forward\n",
    "    y_pred = model(X_t)\n",
    "\n",
    "    # loss\n",
    "    loss = criterion(y_pred, Y_t)\n",
    "    l.append(loss.item())\n",
    "    \n",
    "    # ponemos a cero los gradientes\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backprop (calculamos todos los gradientes automáticamente)\n",
    "    loss.backward()\n",
    "\n",
    "    # update de los pesos\n",
    "    optimizer.step()\n",
    "    \n",
    "    if not e % log_each:\n",
    "        print(f\"Epoch {e}/{epochs} Loss {np.mean(l):.5f}\")\n",
    "        \n",
    "y_pred = evaluate(torch.from_numpy(X_test).float().cuda())\n",
    "accuracy_score(y_test, y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterando por *Batches*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la implementación anterior estamos optimizando nuestro modelo con el algoritmo de `batch gradient descent`, en el que utilizamos todos nuestros datos en cada paso de optimización. Sin embargo, un algoritmo que puede converger más rápido (y única opción si nuestro dataset es tan grande que no cabe en memoria) es el de `mini-batch gradient descent` (el cual hemos ya utilizado en posts anteriores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T13:18:17.723943Z",
     "start_time": "2020-08-15T13:18:13.830897Z"
    },
    "code_folding": [
     17
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Loss 0.31293\n",
      "Epoch 2/10 Loss 0.22047\n",
      "Epoch 3/10 Loss 0.17743\n",
      "Epoch 4/10 Loss 0.15066\n",
      "Epoch 5/10 Loss 0.13168\n",
      "Epoch 6/10 Loss 0.11724\n",
      "Epoch 7/10 Loss 0.10579\n",
      "Epoch 8/10 Loss 0.09652\n",
      "Epoch 9/10 Loss 0.08866\n",
      "Epoch 10/10 Loss 0.08187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.974"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_in, H, D_out = 784, 100, 10\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ").to(\"cuda\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.8)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "log_each = 1\n",
    "l = []\n",
    "model.train()\n",
    "batches = len(X_t) // batch_size\n",
    "for e in range(1, epochs+1): \n",
    "    \n",
    "    _l = []\n",
    "    # iteramos por batches\n",
    "    for b in range(batches):\n",
    "        x_b = X_t[b*batch_size:(b+1)*batch_size]\n",
    "        y_b = Y_t[b*batch_size:(b+1)*batch_size]\n",
    "        \n",
    "        # forward\n",
    "        y_pred = model(x_b)\n",
    "\n",
    "        # loss\n",
    "        loss = criterion(y_pred, y_b)\n",
    "        _l.append(loss.item())\n",
    "\n",
    "        # ponemos a cero los gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backprop (calculamos todos los gradientes automáticamente)\n",
    "        loss.backward()\n",
    "\n",
    "        # update de los pesos\n",
    "        optimizer.step()\n",
    "    \n",
    "    l.append(np.mean(_l))\n",
    "    if not e % log_each:\n",
    "        print(f\"Epoch {e}/{epochs} Loss {np.mean(l):.5f}\")\n",
    "        \n",
    "y_pred = evaluate(torch.from_numpy(X_test).float().cuda())\n",
    "accuracy_score(y_test, y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien esta implementación es correcta y funcional, dependiendo de nuestros datos puede llegar a complicarse mucho (por ejemplo, si necesitamos cargar muchas imágenes a las cuales queremos aplicar transformaciones, juntar en batches, etc...). Además, es común reutilizar la lógica para cargar nuestros datos no sólo para entrenar la red, si no para generar predicciones. Este hecho motiva el uso de las clases especiales que `Pytorch` nos ofrece para ello."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La clase *Dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera clase que tenemos que conocer es la clase `Dataset`. Esta clase hereda de la clase madre `torch.utils.data.Dataset` y tenemos que definir, como mínimo, tres funciones: \n",
    "\n",
    "- `__init__`: el constructor\n",
    "- `__len__`: devuelve el número de muestras en el dataset\n",
    "- `__getitem__`: devuelve una muestra en concreto del dataset\n",
    "\n",
    "Una vez definida la clase, ésta puede usarse como si de cualquier iterador se tratase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T13:21:45.397959Z",
     "start_time": "2020-08-15T13:21:45.390960Z"
    }
   },
   "outputs": [],
   "source": [
    "# clase Dataset, hereda de la clase `torch.utils.data.Dataset`\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    # constructor\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.from_numpy(X).float().cuda()\n",
    "        self.Y = torch.from_numpy(Y).long().cuda()\n",
    "    # devolvemos el número de datos en el dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    # devolvemos el elemento `ix` del dataset\n",
    "    def __getitem__(self, ix):\n",
    "        return self.X[ix], self.Y[ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez definida la clase, podemos instanciar un objeto que podemos usar para iterar por nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T13:21:47.171999Z",
     "start_time": "2020-08-15T13:21:47.074000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset(X_train, y_train)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T13:21:52.775576Z",
     "start_time": "2020-08-15T13:21:48.951127Z"
    },
    "code_folding": [
     2,
     17
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Loss 0.31120\n",
      "Epoch 2/10 Loss 0.21811\n",
      "Epoch 3/10 Loss 0.17516\n",
      "Epoch 4/10 Loss 0.14884\n",
      "Epoch 5/10 Loss 0.13032\n",
      "Epoch 6/10 Loss 0.11630\n",
      "Epoch 7/10 Loss 0.10512\n",
      "Epoch 8/10 Loss 0.09593\n",
      "Epoch 9/10 Loss 0.08819\n",
      "Epoch 10/10 Loss 0.08158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9715"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_in, H, D_out = 784, 100, 10\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ").to(\"cuda\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.8)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "log_each = 1\n",
    "l = []\n",
    "model.train()\n",
    "batches = len(dataset) // batch_size\n",
    "for e in range(1, epochs+1): \n",
    "    \n",
    "    _l = []\n",
    "    # iteramos por batches en el dataset\n",
    "    for b in range(batches):\n",
    "        x_b, y_b = dataset[b*batch_size:(b+1)*batch_size]\n",
    "        \n",
    "        # forward\n",
    "        y_pred = model(x_b)\n",
    "\n",
    "        # loss\n",
    "        loss = criterion(y_pred, y_b)\n",
    "        _l.append(loss.item())\n",
    "\n",
    "        # ponemos a cero los gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backprop (calculamos todos los gradientes automáticamente)\n",
    "        loss.backward()\n",
    "\n",
    "        # update de los pesos\n",
    "        optimizer.step()\n",
    "    \n",
    "    l.append(np.mean(_l))\n",
    "    if not e % log_each:\n",
    "        print(f\"Epoch {e}/{epochs} Loss {np.mean(l):.5f}\")\n",
    "        \n",
    "y_pred = evaluate(torch.from_numpy(X_test).float().cuda())\n",
    "accuracy_score(y_test, y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos iterar directamente sobre el objeto `dataset` de la misma manera que hacíamos anteriormente, sin embargo `Pytorch` no ofrece otro objeto que nos facilita las cosas a la hora de iterar por batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La clase *DataLoader*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase `DataLoader` recibe un `Dataset` e implementa la lógica para iterar nuestros datos en batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T13:18:21.690663Z",
     "start_time": "2020-08-15T13:18:21.675664Z"
    }
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T13:18:21.706666Z",
     "start_time": "2020-08-15T13:18:21.691664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 784]), torch.Size([100]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dataloader))\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También permite mezclar los datos al principio de cada epoch con el parámetro `shuffle`, de manera automática carga nuestros datos de manera optimizada utilizando varios *cores* de nuestra CPU si es posible, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T13:18:33.617368Z",
     "start_time": "2020-08-15T13:18:21.708665Z"
    },
    "code_folding": [
     15,
     19
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Loss 0.30657\n",
      "Epoch 2/10 Loss 0.21561\n",
      "Epoch 3/10 Loss 0.17338\n",
      "Epoch 4/10 Loss 0.14780\n",
      "Epoch 5/10 Loss 0.13006\n",
      "Epoch 6/10 Loss 0.11645\n",
      "Epoch 7/10 Loss 0.10595\n",
      "Epoch 8/10 Loss 0.09697\n",
      "Epoch 9/10 Loss 0.08949\n",
      "Epoch 10/10 Loss 0.08299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9777"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_in, H, D_out = 784, 100, 10\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ").to(\"cuda\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.8)\n",
    "\n",
    "epochs = 10\n",
    "log_each = 1\n",
    "l = []\n",
    "model.train()\n",
    "for e in range(1, epochs+1): \n",
    "    \n",
    "    _l = []\n",
    "    # iteramos por batches en el dataloader\n",
    "    for x_b, y_b in dataloader:\n",
    "        \n",
    "        # forward\n",
    "        y_pred = model(x_b)\n",
    "\n",
    "        # loss\n",
    "        loss = criterion(y_pred, y_b)\n",
    "        _l.append(loss.item())\n",
    "\n",
    "        # ponemos a cero los gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backprop (calculamos todos los gradientes automáticamente)\n",
    "        loss.backward()\n",
    "\n",
    "        # update de los pesos\n",
    "        optimizer.step()\n",
    "    \n",
    "    l.append(np.mean(_l))\n",
    "    if not e % log_each:\n",
    "        print(f\"Epoch {e}/{epochs} Loss {np.mean(l):.5f}\")\n",
    "        \n",
    "y_pred = evaluate(torch.from_numpy(X_test).float().cuda())\n",
    "accuracy_score(y_test, y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También permite definir nuestra propia lógica para crear los batches, algo que puede ser útil en ciertas ocasiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T13:18:33.633369Z",
     "start_time": "2020-08-15T13:18:33.618369Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return torch.stack([x for x, y in batch]), torch.stack([y for x, y in batch]), torch.stack([2.*x for x, y in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T13:18:33.649369Z",
     "start_time": "2020-08-15T13:18:33.634370Z"
    }
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T13:18:52.194888Z",
     "start_time": "2020-08-15T13:18:33.650371Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Loss 0.29201\n",
      "Epoch 2/10 Loss 0.20428\n",
      "Epoch 3/10 Loss 0.16514\n",
      "Epoch 4/10 Loss 0.14113\n",
      "Epoch 5/10 Loss 0.12428\n",
      "Epoch 6/10 Loss 0.11146\n",
      "Epoch 7/10 Loss 0.10123\n",
      "Epoch 8/10 Loss 0.09286\n",
      "Epoch 9/10 Loss 0.08564\n",
      "Epoch 10/10 Loss 0.07947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9746"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_in, H, D_out = 784, 100, 10\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ").to(\"cuda\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.8)\n",
    "\n",
    "epochs = 10\n",
    "log_each = 1\n",
    "l = []\n",
    "model.train()\n",
    "for e in range(1, epochs+1): \n",
    "    \n",
    "    _l = []\n",
    "    # iteramos por batches en el dataloader\n",
    "    # no usamos x2_b, sólo es para ver un ejemplo\n",
    "    for x_b, y_b, x2_b in dataloader:\n",
    "        \n",
    "        # forward\n",
    "        y_pred = model(x_b)\n",
    "\n",
    "        # loss\n",
    "        loss = criterion(y_pred, y_b)\n",
    "        _l.append(loss.item())\n",
    "\n",
    "        # ponemos a cero los gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backprop (calculamos todos los gradientes automáticamente)\n",
    "        loss.backward()\n",
    "\n",
    "        # update de los pesos\n",
    "        optimizer.step()\n",
    "    \n",
    "    l.append(np.mean(_l))\n",
    "    if not e % log_each:\n",
    "        print(f\"Epoch {e}/{epochs} Loss {np.mean(l):.5f}\")\n",
    "        \n",
    "y_pred = evaluate(torch.from_numpy(X_test).float().cuda())\n",
    "accuracy_score(y_test, y_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos visto diferentes maneras en las que podemos iterar por nuestros datos para entrenar un modelo en `Pytorch`. Si nuestro dataset es sencillo y podemos representarlo como un simple `array` de `NumPy` podemos iterar directamente el `array`, transformándolo previamente en un `tensor`. Sin embargo, cuando nuestro dataset sea más grande y no quepa en memoria o necesite cierto pre-proceso o transformaciones, es muy conveniente utilizar las clases que `Pytorch` nos ofrece para ello. Estas clases son, principalmente, el `Dataset` y el `DataLoader`, las cuales nos van a permitir iterar por nuestros datos de manera eficiente y generar *batches* de forma sencilla (además de otras funcionalidades como mezclar los datos al principio de cada epoch, cargar datos en paralelo, etc)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
